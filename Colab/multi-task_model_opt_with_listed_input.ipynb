{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"multi-task_model_opt_with_listed_input.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"xKPj4b4J1dk4"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i54ofDXg1eo-"},"source":["%cd /content/gdrive/My Drive/Optimizasyon"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pzuMsETE1eid"},"source":["!chmod +x Miniconda3-py37_4.8.3-Linux-x86_64.sh\n","!time bash ./Miniconda3-py37_4.8.3-Linux-x86_64.sh -b -f -p /usr/local\n","!time conda config --set always_yes yes --set changeps1 no\n","!time conda install -q -y -c conda-forge python=3.7\n","!time conda install -q -y -c conda-forge rdkit==2020.09.2 \n","import sys\n","sys.path.append('/usr/local/lib/python3.7/site-packages/')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0asQbeSvIqpT"},"source":["import rdkit\n","from rdkit import Chem"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jgQ93LYf1i6W"},"source":["!pip install tensorflow==1.15\n","!pip install torch==1.6.0 torchvision==0.7.0\n","!pip install scipy\n","!pip install keras==2.3.1\n","!pip install sklearn\n","!pip install 'h5py==2.10.0' --force-reinstall"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d4T9JPnK7Go3"},"source":["from keras.layers import Dense, Dropout, Activation, BatchNormalization, Input\n","from keras.models import Model\n","from keras.optimizers import SGD\n","from keras.models import load_model\n","from keras import backend as K\n","from sklearn.utils import class_weight\n","from sklearn.metrics import roc_auc_score\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import log_loss\n","from get_data import GetData\n","\n","import json\n","import pandas as pd\n","import numpy as np\n","import torch\n","import sys\n","import copy\n","import tensorflow as tf\n","\n","sys.path.append('/content/gdrive/My Drive/Optimizasyon')\n","from jtnn import *\n","sys.path.append('/content/gdrive/My Drive/Optimizasyon')\n","\n","vocab = [x.strip(\"\\r\\n \") for x in open(\"/content/gdrive/My Drive/Optimizasyon/unique_canonical_train_vocab.txt\")]\n","vocab = Vocab(vocab)\n","\n","hidden_size = 450\n","latent_size = 56\n","depth = 3\n","stereo = True\n","\n","model_jtvae = JTNNVAE(vocab, hidden_size, latent_size, depth, stereo=stereo)\n","model_jtvae.load_state_dict(torch.load(\"/content/gdrive/My Drive/Optimizasyon/model.iter-9-6000\", map_location=torch.device('cpu')))  # opts.model_path\n","\n","with open('./L1000CDS_subset.json', 'r') as f:\n","    L = json.load(f)\n","    \n","obj = GetData(L=L, cell_line='MCF7', descriptor='jtvae', n_fold=5, random_state=42, random_genes=False, csv_file='JTVAE_Representations.csv')\n","x, y, folds = obj.get_up_genes()\n","trn_x = x.drop(['SMILES'], axis=1).values.astype('float')\n","\n","scaler = StandardScaler()\n","scaler.fit(trn_x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iQPpQKnZAOv4"},"source":["def get_loss(predictions, targets): # cross-entropy\n","    if isinstance(targets, list):\n","        targets = np.asarray(targets)\n","    if isinstance(predictions, list):\n","        predictions = np.asarray(predictions)\n","    N = predictions.shape[0]\n","    return -np.sum((targets*np.log(predictions)) + ((1-targets)*np.log(1-predictions))) / N\n","    \n","def optimize(initial_list, model_up, model_dn, target_up, scaler, target_dn, h=0.00001, lr=0.01, early_stop=5):\n","    initial = scaler.transform(np.asarray(initial_list))\n","    temp = initial\n","    min_loss_updated = copy.deepcopy(temp)\n","    min_ = 999\n","    cnt = 0\n","    gradient = np.zeros((initial.shape[0], 56))\n","    print('h:', h, 'lr:', lr)\n","    \n","    target_up_list = []\n","    target_dn_list = []\n","    for _ in range(initial.shape[0]):\n","        target_up_list.append(target_up)\n","        target_dn_list.append(target_dn)\n","        \n","    print('Up target:', target_up_list[0])\n","    print('Dn target:', target_dn_list[0])\n","\n","    for k in range(0, 1000):\n","        for idx in range(56):\n","            x_1 = copy.deepcopy(temp)\n","            x_2 = copy.deepcopy(temp)\n","            x_1[:, idx] = x_1[:, idx] + h\n","            \n","            pred_list = [x for x in model_up.predict(x_1)]\n","            y_1_up = np.zeros((pred_list[0].shape[0], len(pred_list)))\n","            for i in range(len(pred_list)):\n","                y_1_up[:, i] = pred_list[i].flatten()\n","\n","            pred_list = [x for x in model_up.predict(x_2)]\n","            y_2_up = np.zeros((pred_list[0].shape[0], len(pred_list)))\n","            for i in range(len(pred_list)):\n","                y_2_up[:, i] = pred_list[i].flatten()\n","                \n","            pred_list = [x for x in model_dn.predict(x_1)]\n","            y_1_dn = np.zeros((pred_list[0].shape[0], len(pred_list)))\n","            for i in range(len(pred_list)):\n","                y_1_dn[:, i] = pred_list[i].flatten()\n","                \n","            pred_list = [x for x in model_dn.predict(x_2)]\n","            y_2_dn = np.zeros((pred_list[0].shape[0], len(pred_list)))\n","            for i in range(len(pred_list)):\n","                y_2_dn[:, i] = pred_list[i].flatten()\n","                \n","            for j in range(initial.shape[0]):\n","                loss_1_up = get_loss(predictions=y_1_up[j], targets=target_up_list[j])\n","                loss_2_up = get_loss(predictions=y_2_up[j], targets=target_up_list[j])\n","            \n","                loss_1_dn = get_loss(predictions=y_1_dn[j], targets=target_dn_list[j])\n","                loss_2_dn = get_loss(predictions=y_2_dn[j], targets=target_dn_list[j])\n","            \n","                gradient[j, idx] = ((loss_1_up + loss_1_dn) - (loss_2_up + loss_2_dn)) / h\n","\n","        temp = temp - lr*gradient\n","        \n","        pred_list = [x for x in model_up.predict(temp)]\n","        y_up = np.zeros((pred_list[0].shape[0], len(pred_list)))\n","        for i in range(len(pred_list)):\n","            y_up[:, i] = pred_list[i].flatten()\n","            \n","        pred_list = [x for x in model_dn.predict(temp)]\n","        y_dn = np.zeros((pred_list[0].shape[0], len(pred_list)))\n","        for i in range(len(pred_list)):\n","            y_dn[:, i] = pred_list[i].flatten()\n","        \n","        loss_up = get_loss(predictions=y_up, targets=target_up_list)\n","        loss_dn = get_loss(predictions=y_dn, targets=target_dn_list)\n","        loss = loss_up + loss_dn\n","        \n","        print('Iter:', k+1, 'Loss:', loss)\n","\n","        if min_ > loss:\n","            min_ = loss\n","            min_loss_updated = copy.deepcopy(temp)\n","            cnt = 0\n","        else:\n","            cnt += 1\n","            \n","        if cnt == early_stop:\n","            print(\"Early stopped.\", 'Loss:' + str(loss))\n","            break\n","    \n","    return scaler.inverse_transform(min_loss_updated)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nyvw_9gbMoOL"},"source":["cell_line = 'MCF7'\n","gene_target_up = pd.read_csv('harmonizome_dn_binarized_use_for_up_model.csv')\n","gene_target_dn = pd.read_csv('harmonizome_up_binarized_use_for_dn_model.csv')\n","\n","gene_target_up = gene_target_up[gene_target_up['disease2'] == 'Breast Cancer_3744']\n","gene_target_dn = gene_target_dn[gene_target_dn['disease2'] == 'Breast Cancer_3744']\n","\n","dis_df = pd.read_csv('approved_drug_for_breast_cancer_smiles_jtvae.csv') # MCF7\n","model_up = load_model(cell_line + '_multi_task_model_up.h5')\n","model_dn = load_model(cell_line + '_multi_task_model_dn.h5')\n","\n","file_name_up = cell_line + '_multi_task_gene_list_up.txt'\n","f = open(file_name_up, 'r')\n","lines = f.readlines()\n","gene_list_up = [line.strip() for line in lines]\n","gene_target_up = gene_target_up[gene_list_up].values\n","\n","file_name_dn = cell_line + '_multi_task_gene_list_dn.txt'\n","f = open(file_name_dn, 'r')\n","lines = f.readlines()\n","gene_list_dn = [line.strip() for line in lines]\n","gene_target_dn = gene_target_dn[gene_list_dn].values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Bj8cfYKoMv_P"},"source":["cikarilan_smi_lst = []\n","baslangic_smi_lst = []\n","baslangic_features = []\n","distance_lst = []\n","optimized_smi_lst = []\n","    \n","for idx in range(len(dis_df)):\n","    if (dis_df.values[idx, 2] in cikarilan_smi_lst) or (len(dis_df.values[idx, 2]) > 105):\n","        continue\n","    arr = np.random.uniform(low=-1.5, high=1.5, size=(56,))\n","    baslangic_feat = dis_df.values[idx, 5:].astype('float') + arr\n","    dec_smiles = model_jtvae.reconstruct2(torch.from_numpy(np.asarray([baslangic_feat[0:28]])).float(),\n","                                          torch.from_numpy(np.asarray([baslangic_feat[28:56]])).float())\n","    \n","    cikarilan_smi = dis_df.values[idx, 2]\n","    baslangic_smi = dec_smiles                                \n","    distance = np.linalg.norm(dis_df.values[idx, 5:].astype('float') - baslangic_feat)\n","    optimize_edilen_smi = \"\"\n","    print('Cikarilan Smiles:', cikarilan_smi, 'Baslangic Smiles:', baslangic_smi, 'Distance:', distance)\n","    \n","    cikarilan_smi_lst.append(cikarilan_smi)\n","    baslangic_smi_lst.append(baslangic_smi)\n","    baslangic_features.append(baslangic_feat)\n","    distance_lst.append(distance)\n","    \n","updated_feat = optimize(initial_list=baslangic_features,\n","                        model_up=model_up, model_dn=model_dn, \n","                        target_up=gene_target_up[0],\n","                        target_dn=gene_target_dn[0],\n","                        scaler=scaler)\n","                   \n","for feature in updated_feat:\n","    optimize_edilen_smi = model_jtvae.reconstruct2(torch.from_numpy(np.asarray([feature[0:28]])).float(),\n","                                                   torch.from_numpy(np.asarray([feature[28:56]])).float())\n","                                                   \n","    optimized_smi_lst.append(optimize_edilen_smi)\n","    print('optimized_smiles:', optimize_edilen_smi)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oCAwujtOM5yb"},"source":[""],"execution_count":null,"outputs":[]}]}